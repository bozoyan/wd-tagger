import safetensors.torch
from safetensors import safe_open
import torch
import sys
import os

def patch_final_layer_adaLN(state_dict, prefix="lora_unet_final_layer", verbose=True):
    """
    Add dummy adaLN weights if missing, using final_layer_linear shapes as reference.
    Args:
        state_dict (dict): keys -> tensors
        prefix (str): base name for final_layer keys
        verbose (bool): print debug info
    Returns:
        dict: patched state_dict
    """
    final_layer_linear_down = None
    final_layer_linear_up = None

    adaLN_down_key = f"{prefix}_adaLN_modulation_1.lora_down.weight"
    adaLN_up_key = f"{prefix}_adaLN_modulation_1.lora_up.weight"
    linear_down_key = f"{prefix}_linear.lora_down.weight"
    linear_up_key = f"{prefix}_linear.lora_up.weight"

    if verbose:
        print(f"\nğŸ” Checking for final_layer keys with prefix: '{prefix}'")
        print(f"   Linear down: {linear_down_key}")
        print(f"   Linear up:   {linear_up_key}")

    if linear_down_key in state_dict:
        final_layer_linear_down = state_dict[linear_down_key]
    if linear_up_key in state_dict:
        final_layer_linear_up = state_dict[linear_up_key]

    has_adaLN = adaLN_down_key in state_dict and adaLN_up_key in state_dict
    has_linear = final_layer_linear_down is not None and final_layer_linear_up is not None

    if verbose:
        print(f"   âœ… Has final_layer.linear: {has_linear}")
        print(f"   âœ… Has final_layer.adaLN_modulation_1: {has_adaLN}")

    if has_linear and not has_adaLN:
        # åˆ›å»ºä¸linearæƒé‡ç›¸åŒå½¢çŠ¶çš„adaLNæƒé‡
        # ä½¿ç”¨å°çš„éé›¶å€¼è€Œä¸æ˜¯å®Œå…¨ä¸ºé›¶ï¼Œä»¥ç¡®ä¿æƒé‡è¢«æ­£ç¡®è¯†åˆ«
        dummy_down = torch.randn_like(final_layer_linear_down) * 0.001
        dummy_up = torch.randn_like(final_layer_linear_up) * 0.001
        
        # ç¡®ä¿æƒé‡è¢«æ­£ç¡®æ·»åŠ 
        state_dict[adaLN_down_key] = dummy_down
        state_dict[adaLN_up_key] = dummy_up

        if verbose:
            print(f"âœ… Added adaLN weights with small random values:")
            print(f"   {adaLN_down_key} (shape: {dummy_down.shape})")
            print(f"   {adaLN_up_key} (shape: {dummy_up.shape})")
            print(f"   Note: Using small random values instead of zeros for better compatibility")
    else:
        if verbose:
            print("âœ… No patch needed â€” adaLN weights already present or no final_layer.linear found.")

    return state_dict


def main():
    print("ğŸ”„ Universal final_layer.adaLN LoRA patcher (.safetensors)")
    
    # è®¾ç½®UTF-8ç¼–ç  - ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼
    if sys.platform == "win32":
        try:
            import locale
            # å°è¯•è®¾ç½®UTF-8ç¼–ç ï¼Œå¦‚æœå¤±è´¥åˆ™å¿½ç•¥
            locale.setlocale(locale.LC_ALL, 'C.UTF-8')
        except locale.Error:
            # Windowsä¸æ”¯æŒC.UTF-8ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç 
            pass
    
    try:
        input_path = input("Enter path to input LoRA .safetensors file: ").strip()
        output_path = input("Enter path to save patched LoRA .safetensors file: ").strip()
    except UnicodeDecodeError:
        print("âš ï¸  Encoding error detected. Trying to fix...")
        # å°è¯•ä½¿ç”¨ä¸åŒçš„ç¼–ç æ–¹å¼è¯»å–
        import codecs
        sys.stdin = codecs.getreader('utf-8')(sys.stdin.detach())
        input_path = input("Enter path to input LoRA .safetensors file: ").strip()
        output_path = input("Enter path to save patched LoRA .safetensors file: ").strip()

    # éªŒè¯æ–‡ä»¶è·¯å¾„
    if not os.path.exists(input_path):
        print(f"âŒ Error: Input file not found: {input_path}")
        print(f"   Please check if the file exists and the path is correct.")
        return

    # Load
    state_dict = {}
    try:
        with safe_open(input_path, framework="pt", device="cpu") as f:
            for k in f.keys():
                state_dict[k] = f.get_tensor(k)
    except Exception as e:
        print(f"âŒ Error loading file: {e}")
        return

    print(f"\nâœ… Loaded {len(state_dict)} tensors from: {input_path}")

    # Show all keys that mention 'final_layer' for debug
    final_keys = [k for k in state_dict if "final_layer" in k]
    if final_keys:
        print("\nğŸ”‘ Found these final_layer-related keys:")
        for k in final_keys:
            print(f"   {k}")
    else:
        print("\nâš ï¸  No keys with 'final_layer' found â€” will try patch anyway.")

    # Try common prefixes in order
    prefixes = [
        "lora_unet_final_layer",
        "final_layer",
        "base_model.model.final_layer"
    ]
    patched = False

    for prefix in prefixes:
        before = len(state_dict)
        state_dict = patch_final_layer_adaLN(state_dict, prefix=prefix)
        after = len(state_dict)
        if after > before:
            patched = True
            break  # Stop after the first successful patch

    if not patched:
        print("\nâ„¹ï¸  No patch applied â€” either adaLN already exists or no final_layer.linear found.")

    # Save
    try:
        safetensors.torch.save_file(state_dict, output_path)
        print(f"\nâœ… Patched file saved to: {output_path}")
        print(f"   Total tensors now: {len(state_dict)}")
    except Exception as e:
        print(f"âŒ Error saving file: {e}")
        return

    # Verify
    print("\nğŸ” Verifying patched keys:")
    try:
        with safe_open(output_path, framework="pt", device="cpu") as f:
            keys = list(f.keys())
            for k in keys:
                if "final_layer" in k:
                    print(f"   {k}")

            has_adaLN_after = any("adaLN_modulation_1" in k for k in keys)
            print(f"âœ… Contains adaLN after patch: {has_adaLN_after}")
            
            # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æƒé‡å€¼
            if has_adaLN_after:
                adaLN_keys = [k for k in keys if "adaLN_modulation_1" in k]
                print(f"ğŸ” Found {len(adaLN_keys)} adaLN keys:")
                for k in adaLN_keys:
                    tensor = f.get_tensor(k)
                    print(f"   {k}: shape={tensor.shape}, dtype={tensor.dtype}")
    except Exception as e:
        print(f"âŒ Error verifying file: {e}")


if __name__ == "__main__":
    main()
